# Questionable parts {#ch-questionable-parts}

The JPA, as any ORM, is not without its drawbacks. Firstly, it is complex, much deeper than
developers realize when they approach it. Secondly, it is not a perfect abstraction. The more you
want to play it that way (as a perfect abstraction) the worse it probably gets in marginal cases.
And the margin is not very thin. You may solve 80% cases easily, but there are still 20% of hard
cases where you go around your ORM, write native SQL, etc. If you try to avoid it you'll probably
suffer more than if you accepted it.

You can't just stay on the JPA level, even for cases where ORM works well for you. There are
these details you should know about a provider you use. For instance, let's say you have an entity
with auto-generated identifier based on IDENTITY (or AUTO_INCREMENT) column. You call `persist`
on it and later you want to use its ID somewhere. And it doesn't work, because you're using
EclipseLink and you didn't call `flush` to actually execute that INSERT. Without it the provider
cannot know what value for ID it should use. Maybe your usage of ID value was not the right
*ORM way*, maybe you should have use the whole entity somewhere, but the point is that if you do
the same with Hibernate, it would work. You simply cannot assume that the ID is set.[^demoid]

[^demoid]: You can see this demonstrated in `examples/basic` if you run:
    `mvn test-compile exec:java -Dexec.mainClass="tests.GeneratedIdSettingDemo"`

## Lazy on basic and *to-one* fields {#lazy-problems}

This problem is the reason for this book, without it, I'd probably not go beyond couple of blog
posts. While you can map these fields as lazy, the behaviour is actually not covered properly in
the JPA specification ([[JPspec](#bib-jpspec)]). Its section 2.2, page 25, states:

Q> If property access is used and lazy fetching is specified, portable applications should not
Q> directly access the entity state underlying the property methods of managed instances until
Q> after it has been fetched by the persistence provider.

And the attached footnote adds:

Q> Lazy fetching is a hint to the persistence provider and can be specified by means of the
Q> `Basic`, `OneToOne`, `OneToMany`, `ManyToOne`, `ManyToMany`, and `ElementCollection` annotations
Q> and their XML equivalents.

While ORMs generally have no problem to make collections lazy (e.g. both *to-many* annotations),
for *to-one* mappings this gets more complicated. [[PoEAA](#bib-poeaa)] offers couple of solutions
for *lazy load* pattern: *lazy initialization*, *virtual proxy*, *value holder*, and *ghost*. Not
all are usable for *to-one* mapping.

The essential trouble is that such a field contains an entity directly. There is no indirection
like a collection, that can provide lazy implementation, in case of *to-many* mappings. JPA does
not offer any generic solution for indirect *value holder*. *Virtual proxy* would require some
interface to implement, or byte-code manipulation on the target class, *ghost* would definitely
require byte-code manipulation on the target class, and *lazy initialization* would require
byte-code manipulation, or at least some special implementation, on the source class. JPA design
neither offers any reasonable way how to introduce this indirection without advanced auto-magic
solutions nor ways how to do it explicitly in a way programmer can control.

Removing *to-one* mappings and replace them with raw foreign key values is possible, but this
would disable our ability to join entities in queries -- before *JPA 2.1* came out and brought
`ON` clause. Of course, this was probably meant for additional join conditions, but we can use
it to explicitly state primary-foreign key condition which is normally implied from mapping.
We will expand on this in a [dedicated chapter](#ch-away-from-to-one).


## Generated updates

Programmer using JPA should see the *object* side of the ORM mapping which typically means that
an object is also the granularity on which ORM works. If you change a single attribute on an object
most ORMs simply generate full update for all columns (except for those marked `updatable = false`,
of course). This by itself is probably not such a big deal, but if nothing else, it makes SQL
debug output less useful to check what really changed.

I'd not even expect ORM to eliminate the column from update when it's equal, I'd rather expect
them to include it only when it was set. But we are already in the domain of ORM auto-magic (again)
as they somehow have to know what has changed. Our entities are typically enhanced somehow, either
during the build of a project, or during class-loading. It would probably be more complex to store
touched columns instead of marking the whole entity as "dirty".

All in all, this is just a minor annoyance when you're used to log generated queries, typically
during development, and you simply cannot see what changed among dozens of columns. For this --
and for the cases when you want to change many entities at once in the same way -- you can use
`UPDATE` clause, also known as *bulk update*. But these tend to interfere with caches and with
persistence context. Let's talk about that next.


## Unit of work vs queries

JPA without direct SQL-like capabilities (that is JPQL) would be very limited. Sure, there are
projects you can happily sail through with queries based on criteria API only, but those are the
easy ones. I remember a project, it was an important budgeting system with hierarchical
organizational structure with thousands of organizations. There were budget items and limits for
them with multiple categories, each of them being a hierarchy too. When we needed to recalculate
some items for some category specification (possibly with wildcards) we loaded these items and
then perform these things in memory.

Sometimes it must have been done, rules were simply too complicated for an update. But sometimes
it didn't have to be this way. When the user approved the budget for an organization (and all its
sub-units) we merely needed to set a flag on all these items. That's what you can do with a bulk
update. `UPDATE` and `DELETE` clauses were in the JPA specification since day 0, with the latest
*JPA 2.1* you can do this not only in JPQL, but also in Criteria API.[^capiq]

[^capiq]: We will, however, prefer more expressive Querydsl that generates JPQL.

When you can formulate simple bulk update, you know what to set and `WHERE` to set it, you can
gain massive performance boost. Instead of iterating and generating N updates you just send a
single SQL to the database and that way you can go down from a cycle taking minutes to an operation
taking seconds. But there is one big "but" related to the persistence context (entity manager).
If you have the entities going to be affected by the bulk update in your persistence context, they
will not be touched at all. Bulk updates go around entity manager's "cache" for unit of work, which
means you should not mix bulk updates with modification of entities attached to the persistence
context, unless these are completely separate entities. In general, I try to avoid any complex
logic with attached entities after I execute bulk update/delete -- and typically the scenario does
not require it anyway.

To demonstrate the problem with a snippet of code:

{title="BulkUpdateVsPersistenceContext.java", lang=java}
~~~
System.out.println("dog.name = " + dog.getName()); // Rex

new JPAUpdateClause(em, QDog.dog)
  .set(QDog.dog.name, "Dex")
  .execute();

dog = em.find(Dog.class, 1); // find does not do much here
System.out.println("dog.name = " + dog.getName()); // still Rex

em.refresh(dog); // this reads the real data now
System.out.println("after refresh: dog.name = " + dog.getName()); // Dex
~~~

The same problem applies to JPQL queries that happen after the changes to the entities within a transaction
on the current persistence context. Here the behaviour is controlled by entity manager's flush mode
and it defaults to `FlushModeType.AUTO`.[^flushmode] Flush mode `AUTO` enforces the persistence
context to flush all updates into the database before executing the query. But with flush mode
`COMMIT` you'd get to inconsistencies just like in the scenario with bulk update. Obviously,
flushing the changes is a reasonable option -- you'd flush it sooner or later anyway. Bulk update
scenario, on the other hand, requires us to refresh attached entities which is much more disruptive
and also costly.

[^flushmode]: See [[JPspec](#bib-jpspec)], section 3.10.8.


## Second-level cache vs queries {#cache-vs-queries}

While persistence context (`EntityManager` or session) is sometimes considered a cache too, it
is merely a part of the unit-of-work pattern. The real cache sits underneath and is shared on the
level of the `EntityManagerFactory` -- or even between more of them across various JVMs in case
of distributed caches. This is called the *second-level cache*.[^slc] It is used to enhance
performance, typically by avoiding round-trips to the database. But caching has consequences.

[^slc]: Second-level cache is most popular term, used also in [[JPspec](#bib-jpspec)]. It appears
    in [[PJPA2](#bib-projpa2)] too, but *in-memory cache* is used more often there.

Caching should be transparent, but just turning it on is a kind of *premature optimization* which
-- in virtually all cases -- ends up being wrong. Any auto-magic can only go so far, and any
caching leads to potential inconsistencies. I believe most JPA users don't understand how the cache
is structured (I'm talking from my own experience too, after all). This depends on a concrete ORM,
but typically there is an *entity cache* and a *query cache*.

Entity cache helps with performance of `EntityManager.find`, or generally with loading by entity's
`@Id` attribute. But this will not help you if you accidentally obfuscate what you want with a
query, that would otherwise return the same. The provider has no way to know what entity (with what
ID) will be loaded just looking at arbitrary where conditions. This is what query cache is for.
Bulk update and deletes using JPQL go around either of these caches and the safest way how to
avoid inconsistent data is to evict all entities of the modified type from the caches. This is
often performed by the ORM provider automatically (again, check documentation and settings).

If you only work with whole entities the whole time *and* nothing else accesses the database you
can be pretty sure you always get the right result from the entity cache. You may wonder how this
cache behaves in concurrent environment (like any EE/Spring application inherently is). If you
imagine it as a `Map`, even with synchronized access, you may feel the horror of getting the same
entity instance (`Dog` with the same ID) for two concurrent persistence contexts (like concurrent
HTTP requests) that subsequently modify various fields on the shared instance. Luckily, ORMs
provide each thread with its own copy of the entity. Internally they typically keep entities in
the cache in some "dehydrated" form.[^ecacheorm]

[^ecacheorm]: Tested with Hibernate and EclipseLink.

We will discuss caching again when we talk about [tuning it down](#caching-considerations).


## You can't escape SQL and relation model

Somewhere under cover there is SQL lurking and we better know how it works. We will tackle this
topic with more passion in the second part of the book, but for now we will just demonstrate that
not knowing simply cannot work. Imagine we want a list of all dog owners, and because there is many
of them we want to paginate it. This is 101 of any enterprise application. In JPA we can use
methods `setFirstResult` and `setMaxResults` on `Query` object which corresponds to SQL `OFFSET`
and `LIMIT` clauses.[^pagstand]

[^pagstand]: In many popular databases that is. Standard SQL pagination is quite young and I doubt
  database vendors agreed to implement it.

Let's have a model situation with the following owners (and their dogs): Alan (with dogs Alan,
Beastie and Cessna), Charlie (no dog), Joe (with Rex) and Mike (with Lassie and Dunco). If you
query for the first two owners ordered by name -- pagination without order doesn't make sense --
you'll get Alan and Charlie. However, imagine you want to display names of their dogs in each row.
If you join dogs too, you'll just get Alan twice, courtesy of his many dogs. You may select without
the join and then select dogs for each row, which is our infamous N+1 select problem. This may not
be a big deal for a page of 2, but for 30 or 100 you can see the difference.

These are the effects of the underlying relational model and you can simply escape them. It's not
difficult to deal with them if you accept the relational world underneath. If you fight it, it
fights back.


## Additional layer

Reasoning about common structured, procedural code is quite simple for simple scenarios. We add
higher-level concepts and abstractions to deal with ever more complex problems. When you use JDBC
you know exactly where in the code your SQL is sent to the database, it's easy to control it,
debug it, monitor it, etc. With JPA we are one level higher. You still can try to measure
performance of your queries -- after all typical query is executed where you call it -- but there
are some twists.

First, it can be cached in a query cache -- which may be good if it provides correct results -- but
it also significantly distorts any performance measuring. JPA layer itself takes some time. Query
has to be parsed (add Querydsl serialization to it when used) and entities created and registered
with the persistence context, so they are *managed* as expected. This distorts the result for the
worse, not to mention that for big results it may trigger some additional GC that plain JDBC would
not have.

Best bet is to monitor performance of the SQL on the server itself. Most decent RDBMS provide
some capabilities in this aspect. You can also use JDBC proxy driver that wraps the real one and
performs some logging on the way. Maybe your ORM provides this to a degree, at least in the logs
if nowhere else. This may not be easy to process, but it's still better than no visibility at all.
More sophisticated system may provide nice measuring, but can also add performance penalty -- which
perhaps doesn't affect the monitoring too much, but it can still affect the overall application
performance. Monitoring overhead is, of course, not related to JPA, we would get to it using just
plain JDBC as well.

I will not cover monitoring topics in the book -- they are natural problems with any framework,
although we can argue that access to RDBMS is kinda critical. Unit of work pattern causes real DB
work to happen somewhere else than the domain code would indicate. For simple CRUD-like scenarios
it's not a problem, not even from performance perspective (mostly). For complex scenarios, for
which the pattern was designed in the first place, we may need to revisit what we send to the
database if we encounter performance issues. This may also affect our domain. Maybe there are
clean answers for this, but I don't know them. I typically rather tune down how I use the JPA.

All in all, the fact that JPA adds complexity is very natural aspect of any layer that sits on top
of another layer. This is probably one of the least questionable parts of JPA. You either want it
and accept its "additional layer-ness" or choose not to use it.


## Big unit of work

I believe that *unit of work* pattern is really neat, especially when you have support from tools
like ORM. Even if we don't mix bulk updates and complex persistence contexts, we can still run into
troubles when the context is simply too big. This may easily happen with complicated business
scenarios and it may cause no problem. Often, though, users may see the problem. Request takes too
long or nightly scheduled task is still running late in the morning. Code looks good, it's ORM-ish
as it can be -- it's just slow.

You can monitor or debug how many objects are managed, often you can see effects on the heap. When
this happens something has to change, obviously. Sometimes you can deal with the problem within
your domain model and let ORM shield you from the database. Sometimes it's not possible and you
have to let relational world leak into your code.

I've seen advices how to perform some updates or delete in "more object-oriented manner" -- and
they were probably right from that perspective. But that pulls much more unnecessary data into
memory to perform something that can be expressed easily with JPQL.

D> ### How object-oriented is JPQL?
D>
D> If JPQL is part of JPA, just like other QLs are parts of concrete ORM solutions, then it
D> arguably *is* part of the abstraction. Let's say we have some test cases creating some dog and
D> breed objects. In ideal case we would delete all of them between tests, but as it happens, we
D> are working on database that contains some fixed set of dogs and breeds as well (don't ask).
D> So we mark our test breeds with 'TEST' description. Dog creation is part of tested code, but
D> we know they will be of our testing breeds. To delete all the testing dogs we may then write:
D>
D> ~~~
D> delete from Dog d where d.breed.description = 'TEST'
D> ~~~
D>
D> That's pretty clear JPQL. Besides the fact that it (fails on Hibernate)[https://hibernate.atlassian.net/browse/HHH-9711]
D> it does not touch our persistence context at all and does its job. We can do the same with
D> subquery if we are stuck to Hibernate -- or we can fetch testing breeds into a list (they
D> are now managed) and then execute JPQL like this:
D>
D> ~~~
D> delete from Dog d where d.breed in :breeds
D> ~~~
D>
D> Here `breeds` would be parameter with a list containing the testing breeds as its value. We may
D> fetch a plain list of breed.id instead, this does not get managed, takes less memory and pulls
D> less data from the database with the same effect, we just say `where d.breed.id in :breedIds`
D> instead (if supported like this by your ORM, but it's definitely OK with JPA). I've heard
D> arguments that this is less object-oriented. I was like "what?"
D>
D> Finally, what you can do is start with fetching the testing breeds and then fetch all the
D> dogs with these breeds and call `em.remove(dog)` in a cycle. I hope this, object-oriented as it
D> is, is considered a bit of a stretch by most programmers. But I saw it in teams where JPQL
D> and bulk updates were not very popular (read also as "not part of the knowledge base").


TODO:
* unit-of-work "caching" and tracking a lot of data that are to be viewed
* can be fixed with readonly transactions, nice example of being explicit -- is it possible without Spring?
* reference missing feature for cursor-like result-set "streaming"


## JPA alternatives?

TODO: compare with JDBC, Groovy, Spring JDBC?


## And there is more

This list of potential surprises is far from complete and we will continue in similar tone at
the beginning of the following part.

TODO: mention bugs cross-fire, different providers have different bugs, but all are show-stoppers
for a project:
https://virgo47.wordpress.com/2014/10/09/jpa-is-it-worth-it-horror-stories-with-eclipselink-and-hibernate/