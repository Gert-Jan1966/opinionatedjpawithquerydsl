# Questionable parts {#ch-questionable-parts}

The JPA, as any ORM, is not without its drawbacks. Firstly, it is complex, much deeper than
developers realize when they approach it. Secondly, it is not a perfect abstraction. The more you
want to play it that way (as a perfect abstraction) the worse it probably gets in marginal cases.
And the margin is not very thin. You may solve 80% cases easily, but there are still 20% of hard
cases where you go around your ORM, write native SQL, etc. If you try to avoid it you'll probably
suffer more than if you accepted it.

You can't just stay on the JPA level, even for cases where ORM works well for you. There are
these details you should know about a provider you use. For instance, let's say you have an entity
with auto-generated identifier based on IDENTITY (or AUTO_INCREMENT) column. You call `persist`
on it and later you want to use its ID somewhere. And it doesn't work, because you're using
EclipseLink and you didn't call `flush` to actually execute that INSERT. Without it the provider
cannot know what value for ID it should use. Maybe your usage of ID value was not the right
*ORM way*, maybe you should have use the whole entity somewhere, but the point is that if you do
the same with Hibernate, it would work. You simply cannot assume that the ID is set.[^demoid]

[^demoid]: You can see this demonstrated in `examples/basic` if you run:
    `mvn test-compile exec:java -Dexec.mainClass="tests.GeneratedIdSettingDemo"`

## Lazy on basic and *to-one* fields {#lazy-problems}

This problem is the reason for this book, without it, I'd probably not go beyond couple of blog
posts. While you can map these fields as lazy, the behaviour is actually not covered properly in
the JPA specification ([[JPspec](#bib-jpspec)]). Its section 2.2, page 25, states:

Q> If property access is used and lazy fetching is specified, portable applications should not
Q> directly access the entity state underlying the property methods of managed instances until
Q> after it has been fetched by the persistence provider.

And the attached footnote adds:

Q> Lazy fetching is a hint to the persistence provider and can be specified by means of the
Q> `Basic`, `OneToOne`, `OneToMany`, `ManyToOne`, `ManyToMany`, and `ElementCollection` annotations
Q> and their XML equivalents.

While ORMs generally have no problem to make collections lazy (e.g. both *to-many* annotations),
for *to-one* mappings this gets more complicated. [[PoEAA](#bib-poeaa)] offers couple of solutions
for *lazy load* pattern: *lazy initialization*, *virtual proxy*, *value holder*, and *ghost*. Not
all are usable for *to-one* mapping.

The essential trouble is that such a field contains an entity directly. There is no indirection
like a collection, that can provide lazy implementation, in case of *to-many* mappings. JPA does
not offer any generic solution for indirect *value holder*. *Virtual proxy* would require some
interface to implement, or byte-code manipulation on the target class, *ghost* would definitely
require byte-code manipulation on the target class, and *lazy initialization* would require
byte-code manipulation, or at least some special implementation, on the source class. JPA design
neither offers any reasonable way how to introduce this indirection without advanced auto-magic
solutions nor ways how to do it explicitly in a way programmer can control.

Removing *to-one* mappings and replace them with raw foreign key values is possible, but this
would disable our ability to join entities in queries -- before *JPA 2.1* came out and brought
`ON` clause. Of course, this was probably meant for additional join conditions, but we can use
it to explicitly state primary-foreign key condition which is normally implied from mapping.
We will expand on this in a [dedicated chapter](#ch-away-from-to-one).


## Generated updates

Programmer using JPA should see the *object* side of the ORM mapping which typically means that
an object is also the granularity on which ORM works. If you change a single attribute on an object
most ORMs simply generate full update for all columns (except for those marked `updatable = false`,
of course). This by itself is probably not such a big deal, but if nothing else, it makes SQL
debug output less useful to check what really changed.

I'd not even expect ORM to eliminate the column from update when it's equal, I'd rather expect
them to include it only when it was set. But we are already in the domain of ORM auto-magic (again)
as they somehow have to know what has changed. Our entities are typically enhanced somehow, either
during the build of a project, or during class-loading. It would probably be more complex to store
touched columns instead of marking the whole entity as "dirty".

All in all, this is just a minor annoyance when you're used to log generated queries, typically
during development, and you simply cannot see what changed among dozens of columns. For this --
and for the cases when you want to change many entities at once in the same way -- you can use
`UPDATE` clause, also known as *bulk update*. But these tend to interfere with caches and with
persistence context. Let's talk about that next.


## Unit of work vs queries

JPA without direct SQL-like capabilities (that is JPQL) would be very limited. Sure, there are
projects you can happily sail through with queries based on criteria API only, but those are the
easy ones. I remember a project, it was an important budgeting system with hierarchical
organizational structure with thousands of organizations. There were budget items and limits for
them with multiple categories, each of them being a hierarchy too. When we needed to recalculate
some items for some category specification (possibly with wildcards) we loaded these items and
then perform these things in memory.

Sometimes it must have been done, rules were simply too complicated for an update. But sometimes
it didn't have to be this way. When the user approved the budget for an organization (and all its
sub-units) we merely needed to set a flag on all these items. That's what you can do with a bulk
update. `UPDATE` and `DELETE` clauses were in the JPA specification since day 0, with the latest
*JPA 2.1* you can do this not only in JPQL, but also in Criteria API.[^capiq]

[^capiq]: We will, however, prefer more expressive Querydsl that generates JPQL.

When you can formulate simple bulk update, you know what to set and `WHERE` to set it, you can
gain massive performance boost. Instead of iterating and generating N updates you just send a
single SQL to the database and that way you can go down from a cycle taking minutes to an operation
taking seconds. But there is one big "but" related to the persistence context (entity manager).
If you have the entities going to be affected by the bulk update in your persistence context, they
will not be touched at all. Bulk updates go around entity manager's "cache" for unit of work, which
means you should not mix bulk updates with modification of entities attached to the persistence
context, unless these are completely separate entities. In general, I try to avoid any complex
logic with attached entities after I execute bulk update/delete -- and typically the scenario does
not require it anyway.

To demonstrate the problem with a snippet of code:

{title="BulkUpdateVsPersistenceContext.java", lang=java}
~~~
System.out.println("dog.name = " + dog.getName()); // Rex

new JPAUpdateClause(em, QDog.dog)
  .set(QDog.dog.name, "Dex")
  .execute();

dog = em.find(Dog.class, 1); // find does not do much here
System.out.println("dog.name = " + dog.getName()); // still Rex

em.refresh(dog); // this reads the real data now
System.out.println("after refresh: dog.name = " + dog.getName()); // Dex
~~~

The same problem applies to JPQL queries that happen after the changes to the entities within a transaction
on the current persistence context. Here the behaviour is controlled by entity manager's flush mode
and it defaults to `FlushModeType.AUTO`.[^flushmode] Flush mode `AUTO` enforces the persistence
context to flush all updates into the database before executing the query. But with flush mode
`COMMIT` you'd get to inconsistencies just like in the scenario with bulk update. Obviously,
flushing the changes is a reasonable option -- you'd flush it sooner or later anyway. Bulk update
scenario, on the other hand, requires us to refresh attached entities which is much more disruptive
and also costly.

[^flushmode]: See [[JPspec](#bib-jpspec)], section 3.10.8.


## Second-level cache vs queries {#cache-vs-queries}

While persistence context (`EntityManager` or session) is sometimes considered a cache too, it
is merely a part of the unit-of-work pattern. The real cache sits underneath and is shared on the
level of the `EntityManagerFactory` -- or even between more of them across various JVMs in case
of distributed caches. This is called the *second-level cache*.[^slc] It is used to enhance
performance, typically by avoiding round-trips to the database. But caching has consequences.

[^slc]: Second-level cache is most popular term, used also in [[JPspec](#bib-jpspec)]. It appears
    in [[PJPA2](#bib-projpa2)] too, but *in-memory cache* is used more often there.

Caching should be transparent, but just turning it on is a kind of *premature optimization* which
-- in virtually all cases -- ends up being wrong. Any auto-magic can only go so far, and any
caching leads to potential inconsistencies. I believe most JPA users don't understand how the cache
is structured (I'm talking from my own experience too, after all). This depends on a concrete ORM,
but typically there is an *entity cache* and a *query cache*.

Entity cache helps with performance of `EntityManager.find`, or generally with loading by entity's
`@Id` attribute. But this will not help you if you accidentally obfuscate what you want with a
query, that would otherwise return the same. The provider has no way to know what entity (with what
ID) will be loaded just looking at arbitrary where conditions. This is what query cache is for.
Bulk update and deletes using JPQL go around either of these caches and the safest way how to
avoid inconsistent data is to evict all entities of the modified type from the caches. This is
often performed by the ORM provider automatically (again, check documentation and settings).

If you only work with whole entities the whole time *and* nothing else accesses the database you
can be pretty sure you always get the right result from the entity cache. You may wonder how this
cache behaves in concurrent environment (like any EE/Spring application inherently is). If you
imagine it as a `Map`, even with synchronized access, you may feel the horror of getting the same
entity instance (`Dog` with the same ID) for two concurrent persistence contexts (like concurrent
HTTP requests) that subsequently modify various fields on the shared instance. Luckily, ORMs
provide each thread with its own copy of the entity. Internally they typically keep entities in
the cache in some "dehydrated" form.[^ecacheorm]

[^ecacheorm]: Tested with Hibernate and EclipseLink.

We will discuss caching again when we talk about [tuning it down](#caching-considerations).


## You can't escape SQL and relation model

Somewhere under cover there is SQL lurking and we better know how it works. We will tackle this
topic with more passion in the second part of the book, but for now we will just demonstrate that
not knowing simply cannot work. Imagine we want a list of all dog owners, and because there is many
of them we want to paginate it. This is 101 of any enterprise application. In JPA we can use
methods `setFirstResult` and `setMaxResults` on `Query` object which corresponds to SQL `OFFSET`
and `LIMIT` clauses.[^pagstand]

[^pagstand]: In many popular databases that is. Standard SQL pagination is quite young and I doubt
  database vendors agreed to implement it.

Let's have a model situation with the following owners (and their dogs): Alan (with dogs Alan,
Beastie and Cessna), Charlie (no dog), Joe (with Rex) and Mike (with Lassie and Dunco). If you
query for the first two owners ordered by name -- pagination without order doesn't make sense --
you'll get Alan and Charlie. However, imagine you want to display names of their dogs in each row.
If you join dogs too, you'll just get Alan twice, courtesy of his many dogs. You may select without
the join and then select dogs for each row, which is our infamous N+1 select problem. This may not
be a big deal for a page of 2, but for 30 or 100 you can see the difference.

These are the effects of the underlying relational model and you can simply escape them. It's not
difficult to deal with them if you accept the relational world underneath. If you fight it, it
fights back.


## Additional layer

Other problems:
* monitoring of SQL from JPA application is difficult (it's kinda "elsewhere") and the
interaction with the SQL itself has a lot of overhead as well (but this is complex topic anyway,
JDBC proxy driver is possibility, but how to connect to the business logic or place where it
happens?)


## Big unit of work

Let's get back to unit of work again. I believe that this pattern is really neat, especially when
you have support from tools like ORM. Even if we don't mix bulk updates and complex persistence
contexts, we can still run into troubles when the context is simply too big.
TODO:
* unit-of-work "caching" and tracking a lot of data that are to be viewed
* can be fixed with readonly transactions, nice example of being explicit -- is it possible without Spring?
* reference missing feature for cursor-like result-set "streaming"


## And there is more

This list of potential surprises is far from complete and we will continue in similar tone at
the beginning of the following part.

TODO: mention bugs cross-fire, different providers have different bugs, but all are show-stoppers
for a project
