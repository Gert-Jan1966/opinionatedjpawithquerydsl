# Questionable parts {#ch-questionable-parts}

The JPA, as any ORM, is not without its drawbacks. Firstly, it is complex -- much deeper than
developers realize when they approach it. Secondly, it is not a perfect abstraction. The more you
want to play it that way (as a perfect abstraction) the worse it probably gets in marginal cases.
And the margin is not very thin. You may solve 80% cases easily, but there are still 20% of hard
cases where you go around your ORM, write native SQL, etc. If you try to avoid it you'll probably
suffer more than if you accepted it.

You can't just stay on the JPA level, even for cases where ORM works well for you. There are
these details you should know about a provider you use. For instance, let's say you have an entity
with auto-generated identifier based on IDENTITY (or AUTO_INCREMENT) column. You call `persist`
on it and later you want to use its ID somewhere. And it doesn't work, because you're using
EclipseLink and you didn't call `flush` to actually execute that INSERT. Without it the provider
cannot know what value for ID it should use. Maybe your usage of ID value was not the right
*ORM way*, maybe you should have use the whole entity somewhere, but the point is that if you do
the same with Hibernate, it would work. You simply cannot assume that the ID is set.[^demoid]

[^demoid]: You can see this demonstrated in `examples/basic` if you run:
    `mvn test-compile exec:java -Dexec.mainClass="tests.GeneratedIdSettingDemo"`

## Lazy on basic and *to-one* fields {#lazy-problems}

This problem is the reason for this book, without it, I'd probably not go beyond couple of blog
posts. While you can map these fields as lazy, the behaviour is actually not guaranteed according
to the JPA specification ([[JPspec](#bib-jpspec)]). Its section 2.2, page 25, states:

Q> If property access is used and lazy fetching is specified, portable applications should not
Q> directly access the entity state underlying the property methods of managed instances until
Q> after it has been fetched by the persistence provider.

And the attached footnote adds:

Q> Lazy fetching is a hint to the persistence provider and can be specified by means of the
Q> `Basic`, `OneToOne`, `OneToMany`, `ManyToOne`, `ManyToMany`, and `ElementCollection` annotations
Q> and their XML equivalents.

While ORMs generally have no problem to make collections lazy (e.g. both *to-many* annotations),
for *to-one* mappings this gets more complicated. [[PoEAA](#bib-poeaa)] offers couple of solutions
for *lazy load* pattern: *lazy initialization*, *virtual proxy*, *value holder*, and *ghost*. Not
all are usable for *to-one* mapping.

The essential trouble is that such a field contains an entity directly. There is no indirection
like a collection, that can provide lazy implementation, in case of *to-many* mappings. JPA does
not offer any generic solution for indirect *value holder*. *Virtual proxy* would require some
interface to implement, or bytecode manipulation on the target class, *ghost* would definitely
require bytecode manipulation on the target class, and *lazy initialization* would require
bytecode manipulation, or at least some special implementation, on the source class. JPA design
neither offers any reasonable way how to introduce this indirection without advanced auto-magic
solutions nor ways how to do it explicitly in a way programmer can control.

Removing *to-one* mappings and replace them with raw foreign key values is currently not possible
with pure JPA even though *JPA 2.1* brought `ON` clause for JPQL but it does not allow root
entities in `JOIN`s. We will expand on this in the chapter
[Troubles with *to-one* relationships](#ch-to-one-troubles).


## Generated updates

Programmer using JPA should see the *object* side of the ORM mapping which typically means that
an object is also the granularity on which ORM works. If you change a single attribute on an object
ORM can simply generate full update for all columns (except for those marked `updatable = false`,
of course). This by itself is probably not such a big deal performance-wise, but if nothing else
it makes SQL debug output less useful to check what really changed.

I'd not even expect ORM to eliminate the column from update when it's equal, I'd rather expect
them to include it only when it was set. But we are already in the domain of ORM auto-magic (again)
as they somehow have to know what has changed. Our entities are typically enhanced somehow, either
during the build of a project, or during class-loading. It would probably be more complex to store
touched columns instead of marking the whole entity as "dirty".

To be concrete, EclipseLink out of the box updates only modified attributes/columns, while
Hibernate updates all of them except for ID which is part of the `WHERE` clause.[^el-hib-upd]
We may even argue what is better and when. If you load an object in some state, change a single
column and then commit the transaction, do we really want to follow the changes per attribute or do
we expect the whole object to be "transferred" into the database as-is at the moment of the commit?
If we don't squeeze performance and our transactions are short-lived (and they better are for most
common applications) there is virtually no difference from the consistency point either.

[^el-hib-upd]: Run the code from [this GitHub example](https://github.com/virgo47/opinionatedjpawithquerydsl/blob/master/examples/querydsl-basic/src/test/java/tests/PartialUpdateDemo.java)
    and see the console output.

All in all, this is just a minor annoyance when you're used to log generated queries, typically
during development, and you simply cannot see what changed among dozens of columns. For this --
and for the cases when you want to change many entities at once in the same way -- you can use
`UPDATE` clause, also known as *bulk update*. But these tend to interfere with caches and with
persistence context. Let's talk about that next.


## Unit of work vs queries

JPA without direct SQL-like capabilities (that is JPQL) would be very limited. Sure, there are
projects you can happily sail through with queries based on criteria API only, but those are the
easy ones. I remember a project, it was an important budgeting system with hierarchical
organizational structure with thousands of organizations. There were budget items and limits for
them with multiple categories, each of them being a hierarchy too. When we needed to recalculate
some items for some category specification (possibly with wildcards) we loaded these items and
then perform these things in memory.

Sometimes it must have been done, rules were simply too complicated for an update. But sometimes
it didn't have to be this way. When the user approved the budget for an organization (and all its
sub-units) we merely needed to set a flag on all these items. That's what you can do with a bulk
update. `UPDATE` and `DELETE` clauses were in the JPA specification since day 0, with the latest
*JPA 2.1* you can do this not only in JPQL, but also in Criteria API.[^capiq]

[^capiq]: We will, however, prefer more expressive Querydsl that generates JPQL.

When you can formulate simple bulk update, you know what to set and `WHERE` to set it, you can
gain massive performance boost. Instead of iterating and generating N updates you just send a
single SQL to the database and that way you can go down from a cycle taking minutes to an operation
taking seconds. But there is one big "but" related to the persistence context (entity manager).
If you have the entities going to be affected by the bulk update in your persistence context, they
will not be touched at all. Bulk updates go around entity manager's "cache" for unit of work, which
means you should not mix bulk updates with modification of entities attached to the persistence
context, unless these are completely separate entities. In general, I try to avoid any complex
logic with attached entities after I execute bulk update/delete -- and typically the scenario does
not require it anyway.

To demonstrate the problem with a snippet of code:

{title="BulkUpdateVsPersistenceContext.java", lang=java}
~~~
System.out.println("dog.name = " + dog.getName()); // Rex

new JPAUpdateClause(em, QDog.dog)
  .set(QDog.dog.name, "Dex")
  .execute();

dog = em.find(Dog.class, 1); // find does not do much here
System.out.println("dog.name = " + dog.getName()); // still Rex

em.refresh(dog); // this reads the real data now
System.out.println("after refresh: dog.name = " +
  dog.getName()); // Dex
~~~

The same problem applies to JPQL queries which happen after the changes to the entities within
a transaction on the current persistence context. Here the behaviour is controlled by entity
manager's flush mode and it defaults to `FlushModeType.AUTO`.[^flushmode] Flush mode `AUTO`
enforces the persistence context to flush all updates into the database before executing the query.
But with flush mode `COMMIT` you'd get to inconsistencies just like in the scenario with bulk
update. Obviously, flushing the changes is a reasonable option -- you'd flush it sooner or later
anyway. Bulk update scenario, on the other hand, requires us to refresh attached entities which
is much more disruptive and also costly.

[^flushmode]: See [[JPspec](#bib-jpspec)], section 3.10.8.


## Second-level cache vs queries {#cache-vs-queries}

While persistence context (`EntityManager` or session) is sometimes considered a cache too, it
is merely a part of the unit-of-work pattern. The real cache sits underneath and is shared on the
level of the `EntityManagerFactory` -- or even between more of them across various JVMs in case
of distributed caches. This is called the *second-level cache*.[^slc] It is used to enhance
performance, typically by avoiding round-trips to the database. But caching has consequences.

[^slc]: Second-level cache is most popular term, used also in [[JPspec](#bib-jpspec)]. It appears
    in [[PJPA2](#bib-projpa2)] too, but *in-memory cache* is used more often there.

Caching should be transparent, but just turning it on is a kind of *premature optimization* which
-- in virtually all cases -- ends up being wrong. Any auto-magic can only go so far, and any
caching leads to potential inconsistencies. I believe most JPA users don't understand how the cache
is structured (I'm talking from my own experience too, after all). This depends on a concrete ORM,
but typically there is an *entity cache* and a *query cache*.

Entity cache helps with performance of `EntityManager.find`, or generally with loading by entity's
`@Id` attribute. But this will not help you if you accidentally obfuscate what you want with a
query, that would otherwise return the same. The provider has no way to know what entity (with what
ID) will be loaded just looking at arbitrary where conditions. This is what query cache is for.
Bulk update and deletes using JPQL go around either of these caches and the safest way how to
avoid inconsistent data is to evict all entities of the modified type from the caches. This is
often performed by the ORM provider automatically (again, check documentation and settings).

If you only work with whole entities the whole time *and* nothing else accesses the database you
can be pretty sure you always get the right result from the entity cache. You may wonder how this
cache behaves in concurrent environment (like any EE/Spring application inherently is). If you
imagine it as a `Map`, even with synchronized access, you may feel the horror of getting the same
entity instance (`Dog` with the same ID) for two concurrent persistence contexts (like concurrent
HTTP requests) that subsequently modify various fields on the shared instance. Luckily, ORMs
provide each thread with its own copy of the entity. Internally they typically keep entities in
the cache in some "dehydrated" form.[^ecacheorm]

[^ecacheorm]: Tested with Hibernate and EclipseLink.

We will discuss caching again when we talk about [tuning it down](#caching-considerations).


## You can't escape SQL and relation model {#cant-escape-sql}

Somewhere under cover there is SQL lurking and we better know how it works. We will tackle this
topic with more passion in the second part of the book, but for now we will just demonstrate that
not knowing simply cannot work. Imagine we want a list of all dog owners, and because there is many
of them we want to paginate it. This is 101 of any enterprise application. In JPA we can use
methods `setFirstResult` and `setMaxResults` on `Query` object which corresponds to SQL `OFFSET`
and `LIMIT` clauses.[^pagstand]

[^pagstand]: In many popular databases that is. Standard SQL pagination is quite young and I doubt
  database vendors agreed to implement it.

Let's have a model situation with the following owners (and their dogs): Alan (with dogs Alan,
Beastie and Cessna), Charlie (no dog), Joe (with Rex) and Mike (with Lassie and Dunco). If you
query for the first two owners ordered by name -- pagination without order doesn't make sense --
you'll get Alan and Charlie. However, imagine you want to display names of their dogs in each row.
If you join dogs too, you'll just get Alan twice, courtesy of his many dogs. You may select without
the join and then select dogs for each row, which is our infamous N+1 select problem. This may not
be a big deal for a page of 2, but for 30 or 100 you can see the difference. We will talk about
this particular problem later [in the chapter about N+1](#to-many-paginating).

These are the effects of the underlying relational model and you can simply escape them. It's not
difficult to deal with them if you accept the relational world underneath. If you fight it, it
fights back.


## Additional layer

Reasoning about common structured, procedural code is quite simple for simple scenarios. We add
higher-level concepts and abstractions to deal with ever more complex problems. When you use JDBC
you know exactly where in the code your SQL is sent to the database, it's easy to control it,
debug it, monitor it, etc. With JPA we are one level higher. You still can try to measure
performance of your queries -- after all typical query is executed where you call it -- but there
are some twists.

First, it can be cached in a query cache -- which may be good if it provides correct results -- but
it also significantly distorts any performance measuring. JPA layer itself takes some time. Query
has to be parsed (add Querydsl serialization to it when used) and entities created and registered
with the persistence context, so they are *managed* as expected. This distorts the result for the
worse, not to mention that for big results it may trigger some additional GC that plain JDBC would
not have.

Best bet is to monitor performance of the SQL on the server itself. Most decent RDBMS provide
some capabilities in this aspect. You can also use JDBC proxy driver that wraps the real one and
performs some logging on the way. Maybe your ORM provides this to a degree, at least in the logs
if nowhere else. This may not be easy to process, but it's still better than no visibility at all.
More sophisticated system may provide nice measuring, but can also add performance penalty -- which
perhaps doesn't affect the monitoring too much, but it can still affect the overall application
performance. Monitoring overhead is, of course, not related to JPA, we would get to it using just
plain JDBC as well.

I will not cover monitoring topics in the book -- they are natural problems with any framework,
although we can argue that access to RDBMS is kinda critical. Unit of work pattern causes real DB
work to happen somewhere else than the domain code would indicate. For simple CRUD-like scenarios
it's not a problem, not even from performance perspective (mostly). For complex scenarios, for
which the pattern was designed in the first place, we may need to revisit what we send to the
database if we encounter performance issues. This may also affect our domain. Maybe there are
clean answers for this, but I don't know them. I typically rather tune down how I use the JPA.

All in all, the fact that JPA adds complexity is very natural aspect of any layer that sits on top
of another layer. This is probably one of the least questionable parts of JPA. You either want it
and accept its "additional layer-ness" or choose not to use it.


## Big unit of work

I believe that *unit of work* pattern is really neat, especially when you have support from tools
like ORM. Even if we don't mix bulk updates and complex persistence contexts, we can still run into
troubles when the context is simply too big. This may easily happen with complicated business
scenarios and it may cause no problem. Often, though, users may see the problem. Request takes too
long or nightly scheduled task is still running late in the morning. Code looks good, it's ORM-ish
as it can be -- it's just slow.

You can monitor or debug how many objects are managed, often you can see effects on the heap. When
this happens something has to change, obviously. Sometimes you can deal with the problem within
your domain model and let ORM shield you from the database. Sometimes it's not possible and you
have to let relational world leak into your code.

D> ### How object-oriented is JPQL?
D>
D> I've seen advices how to perform some updates or deletes in "more object-oriented manner" -- and
D> they were probably right from that perspective. But that often pulls a lot of unnecessary data
D> into memory to perform something that can be expressed easily with JPQL. If JPQL is part of JPA,
D> just like other QLs are parts of concrete ORM solutions, then it arguably *is* part of the
D> abstraction. Let's examine various options in the following example.

Let's say we have some test cases creating some dog and breed objects. In ideal case we would
delete all of them between tests, but as it happens, we are working on database that contains
some fixed set of dogs and breeds as well (don't ask). So we mark our test breeds with 'TEST'
description. Dog creation is part of tested code, but we know they will be of our testing breeds.
To delete all the testing dogs we may then write:

~~~
delete from Dog d where d.breed.description = 'TEST'
~~~

That's pretty clear JPQL. Besides the fact that it [fails on Hibernate](https://hibernate.atlassian.net/browse/HHH-9711)
it does not touch our persistence context at all and does its job. We can do the same with
subquery (works for Hibernate as well) -- or we can fetch testing breeds into a list (they
are now managed by the persistence context) and then execute JPQL like this:

~~~
delete from Dog d where d.breed in :breeds
~~~

Here `breeds` would be parameter with a list containing the testing breeds as its value. We may
fetch a plain list of breed.id instead, this does not get managed, takes less memory and pulls
less data from the database with the same effect, we just say `where d.breed.id in :breedIds`
instead -- if supported like this by your ORM, but it's definitely OK with JPA. I've heard
arguments that this is less object-oriented. I was like "what?"

Finally, what you can do is start with fetching the testing breeds and then fetch all the
dogs with these breeds and call `em.remove(dog)` in a cycle. I hope this, object-oriented as it
is, is considered a bit of a stretch even by OO programmers. But I saw it in teams where JPQL
and bulk updates were not very popular (read also as "not part of the knowledge base").

W> In most cases the persistence context is short-lived, so even when a lot of data flows through,
W> it will probably not be a big deal for the
W> [garbage collector](https://en.wikipedia.org/wiki/Garbage_collection_%28computer_science%29) --
W> unless you seriously overflow young generation. But it will affect CPU load, network bandwidth
W> -- as the "object-oriented" approach likely goes to the database multiple times -- and/or
W> caches.
W>
W> If you cache a lot, it will affect application's memory footprint and garbage collector, and if
W> the cache is distributed it may still trigger network roundtrips. These may or may not be
W> quicker than the database access, depending on how much you can get by id; to re-hydrate a
W> cached entity should be faster than to fetch it from the database. I hope you can see a lot of
W> trade-offs you *should* think about when you choose any more sophisticated path -- this includes
W> paths that seem easy at their beginning only because they are covered with a veil of auto-magic
W> solutions. We will get to this when we talk about [Vietnam of Computer Science](#vietnam).

Persistence context (unit of work) has a lot to do when the transaction is committed. It needs to
be flushed which means it needs to check all the *managed* (or attached) entities, and figure out
whether they are changed or not (dirty checking). How the dirty checking is performed is beyond the
scope of this book, typically some bytecode manipulation is used. The problem is that this takes
longer when the persistence context is big. When it's big for a reason, then you have to do what
you have to do, but when it's big because we didn't harness bulk updates/deletes, than it's simply
wasteful. Often it also takes a lot of code when a simple JPQL says the same. I don't accept an
answer that reading JPQL or SQL is hard. If we use RDBMS we *must* know it reasonably well.

Sometimes bulk update is not a feasible option, but you still just want to loop through some
result, modify the entity, flush it (or batch the update if possible) but you don't want it
managed. This relates to the missing "cursor-like result set streaming" mentioned in the section
covering [missing features](#missing-from-orm) some ORM providers do have. You just want to do
something on the entity as you loop through the results

D> ### Do I need transactions for reading?
D>
D> This is a tricky one. Maybe not, but I encountered strange things happening when use cases
D> loaded stale data. This happened when we used Spring and its `@Transactional` annotation.
D> Without it I didn't see the changes from previously finished transactions. There was no race
D> condition involved, it was all nice slow clicking in a CRUD-like web application. Adding
D> `@Transactional` to the service reading data "magically" fixed it. I'm sorry I didn't have time
D> to dig deeper than, but I remember it vividly. So just know, it may happen.
D>
D> But if you make read-only scenario transactional you add all the burden we've just described
D> before. Spring offers attribute on the annotation to mark the transaction as read-only. This
D> disables dirty checking on the persistence context which makes sense. Unfortunately, if you
D> accidentally use some modification SQL via bulk update/delete or even going down to JDBC, it
D> may get committed, as not all JDBC drivers respect `connection.setReadOnly(true)`. While it's
D> an obvious bug (or missing feature, whatever), marking transaction as read-only does *not*
D> guarantee that nothing gets committed. It still says clearly your intention and it *should*
D> bring some time-saving on the *unit of work* level. Unfortunately, again, this does not work
D> universally, not even on the ORM level. Hibernate's session does not get flushed, but I'm
D> pretty sure EclipseLink flushes updates to the database on the JPA level and does not set
D> JDBC connection to [read-only either](https://jira.spring.io/browse/SPR-7891).
D>
D> I don't know how to mark transaction as read-only with JTA `@Transactional` annotation.
D>
D> I highly recommend reading older but still very relevant article [Transaction strategies:
D> Understanding transaction pitfalls](http://www.ibm.com/developerworks/java/library/j-ts1/)


## JPA alternatives?

When JPA suddenly stands in your way instead of helping you, you can still fallback gracefully
to JDBC level. Personally I don't mix this within a single scenario, but you can. For that you
have to know what provider you use, unwrap its concrete implementation of `EntityManager` and ask
it for a `java.sql.Connection` in non-portable manner. When I don't mix scenarios, I simply ask
Spring (or possibly other container) to inject underlying `javax.sql.DataSource` and then I can
access `Connection` without using JPA at all. Talking about Spring, I definitely go for their
`JdbcTemplate` to avoid all the JDBC boilerplate. Otherwise I prefer JPA for all the reasons
mentioned in the [Good Parts](#ch-good-parts).

We've lightly compared JPA with concrete ORMs already, but they are still the same concept -- it
is much more fun to compare it with something else altogether, let's say a different language --
like [Groovy](http://www.groovy-lang.org/). We're still firmly on JVM although it's not very likely
to do your persistence in Groovy and the rest of the application in Java. Firstly, Groovy language
also has its ORM. It's called [GORM](https://grails.github.io/grails-data-mapping/latest/) and
while not built into the core project it is part of [Grails framework](https://grails.org/). I
don't have any experience with it, but I don't expect radical paradigm shift as it uses Hibernate
ORM to access RDBMS (although it supports also No-SQL solutions). Knowing Groovy I'm sure it brings
some fun into the mix, but it still is ORM.

I often use core Groovy [support for relation databases](http://www.groovy-lang.org/databases.html)
and I really like it. It is no ORM, but it makes working with database very easy compared to
JDBC. I readily use it to automate data population as it is much more expressive than SQL
statements -- you mostly just use syntax based on Groovy Maps. With little support code you create
helper insert/update methods that can provide reasonable defaults for columns you don't want to
specify every time or insert whole aggregates (master-slave table structures). It's very easy to
use returned auto-increment primary keys (as named variables, of course) where you need to use
them as foreign keys and it's also very easy to create repetitive data in a loop.

I use this basic database support in Groovy even on projects where I already have entities in JPA,
but for whatever reason I don't want to use them. You actually don't need to map anything into
objects, mostly couple of methods will do. Sure you have to rename columns in the code when you
refactor, but column names are hidden in just a couple places, often just a single one. Bottom
line? Very convenient, natural type conversion (although not perfect, mind you), little to no
boilerplate code and it all plays nicely with Groovy syntax. It definitely doesn't bring so much
negative passion with it as JPA does -- perhaps because the level of sophistication is not so high.
But in many cases even simple solutions can deliver.


## Love and hate for ORM

As I said already, ORM is typically seen as a good fit with domain-driven design (DDD). But
ORM happened to become extremely popular and people thought it will solve all of their database
access problems without the need to learn SQL. This way obviously failed and hurt ORM back a lot,
too. I say it again, ORM is very difficult, even using well documented ORM (like the JPA standard)
is hard -- there's simply too much in it. We're dealing with complex problem, with a mismatch --
and some kind of mismatch it is. And it's not the basic principle that hurts, we always get burnt
on many, too many, details.


### Vietnam of Computer Science {#vietnam}

One of the best balanced texts that critique ORM, and probably one of the most famous, is quite
old actually. In 2006 Ted Neward wrote an extensive post with a fitting, if provoking, name
[The Vietnam of Computer Science](http://blogs.tedneward.com/post/the-vietnam-of-computer-science/).
If you seriously want to use ORM on any of your projects, you should read this -- unless data
access is not an important part of that project (who are we kidding, right?). You may skip the
history of Vietnam war, but definitely give the technical part a dive it deserves.

ORM wasn't that young anymore in 2006 and various experiences had shown that it easily brought more
problems than benefits -- especially if approached with partial knowledge, typically based on
an assumption that "our developers don't need to know SQL". When kind of non-programming architect
recommends this for a project and they are long gone when the trouble appears it's really easy to
recommend it again. It was so easy to generate entities from our [DDL](https://en.wikipedia.org/wiki/Data_definition_language),
wasn't it? Hopefully managers are too high to be an audience for JPA/ORM recommendations,
but technical guys can hurt themselves well enough. The JPA, for instance, is still shiny, it's a
standard after all -- and yeah, it is kinda Java EE-ish, but this is the good new EE, right?

Wrong. Firstly, JPA/ORM is so complex when it comes to details, that using it as a tool for "cheap
developers" who don't need to learn SQL is as silly as it gets. I don't know when learning became
the bad thing in the first place, but some managers think they can save man-months/time/money
without learning. When the things get messy -- and they will -- there is nobody around who really
understands ORM and there is virtually no chance to rewrite data access layer to get rid of it.
Easiest thing to do is to blame ORM.

You may ask: What has changed since 2006? My personal take on the answer would be:

* Nothing essential could have changed, we merely smoothed some rough edges, got a bit more
familiar with already familiar topic and in Java space we standardized the beast (JPA).
* We added more options to query languages to make the gap with SQL smaller. Funny enough, the JPA
actually didn't help here as it lagged couple of years behind capabilities of the leading ORM
solutions.
* Query-by-API (mentioned in the post) is much better nowadays, state of the art technologies like
Querydsl have very rich fluent API that is also very compact (definitely not "much more verbose
than the traditional SQL approach"). Also both type safety and testing practices are much more
developed.

Other than that, virtually all the concerns Ted mentioned are still valid.


### Not much love for ORM

Whatever was written back in 2006, ORMs were on the rise since then. Maybe the absolute numbers of
ORM experts are now higher than then, but I'd bet the ratio of experts among its users plummeted
(though I have no research to support this). ORM/JPA is easily available, Java EE supports it,
Spring supports it, you can use it in Java SE easily, you can even generate CRUD scaffolding for
your application with JPA using some [rapid application development](https://en.wikipedia.org/wiki/Rapid_application_development)
tools. That means a lot of developers are exposed to it. In many cases it seems deceivingly easy
when you start using it.

ORM has a bad reputation with our [DBAs](https://en.wikipedia.org/wiki/Database_administrator) --
and for good reasons too. It takes some effort to make it use reasonable queries, not to mention
that you typically have to break the ORM abstraction to do so. It's good to start with clean,
untangled code as it helps tremendously when you need to optimize some queries later. This,
however, causes many complications. If you explicitly name columns and don't load whole entities
you may get better performance, but it will be unfriendly to the entity cache. The same goes for
bulk updates (e.g. "change this column for all entities where..."). There are the right ways to
do it in the domain model, but learning the right path of OO and domain-driven design is probably
even harder than starting with JPA -- otherwise we'd see many more DDD-based projects around.

We will tackle caching later in this chapter, but I'd say that misunderstandings around ORM caching
are the reason for a lot of performance problems and potentially for data corruption too. This is
when it starts to hurt -- and when you can't get easily out, hate often comes with it. When you
make a mistake with a UI library, you may convince someone to let you rewrite it -- and they can
see the difference. Rewriting data access layer gives seemingly nothing to the client, unless the
data access is really slow, but then the damage done is already quite big anyway.


### But a lot of hate

When you need to bash some technology, ORM is a safe bet. I don't know whether I should even
mention [ORM Is an Offensive Anti-Pattern](http://www.yegor256.com/2014/12/01/orm-offensive-anti-pattern.html)
but as it is now the top result on Google search for ORM, I do it anyway. It wouldn't be fair to
say the author doesn't provide an alternative, but I had read a lot of his other posts (before I
stopped) to see where "SQL-speaking objects" are going to. I cannot see
[single responsibility principle](https://en.wikipedia.org/wiki/Single_responsibility_principle)
in it at all, and while SRP doesn't have to be the single holy grail of OOP, putting everything
into the domain object itself is not a good idea. In different post, domain objects were "enriched"
with all kinds of serialization needed, effectively binding possibly all domain objects to possibly
all technologies you need. I believe OOP has been there already, maybe not before I was born, but
definitely long before I started programming.

There are other flaws with this particular post. Mapping is explained on a very primitive case,
while ORM utilizes unit-of-work for cases where you want to execute multiple updates in a single
transaction, possibly updates on the same object. If every elementary change on the object emits
an SQL and we want to set many properties for the same underlying table in a transaction we get
performance even worse than non-tuned ORM! You can answer with object exposing various methods
to update this and that, which possibly leads to a
[combinatorial explosion](https://en.wikipedia.org/wiki/Combinatorial_explosion). Further, in times
of injection we are shown the most verbose way how to do ORM, the way I haven't seen for years.

"SQL-speaking objects" bring us to a more generic topic of modelling objects in our programs. We
hardly model real-life objects as they act in real life, because in many cases we should not.
Information systems allow changing data about things that normally cannot change, because someone
might have entered the information incorrectly in the first place.

How to model the behavior of a tin can? Should it have `open` method? Even in real life *someone*
opens the *tin* with a *tin opener* -- an interaction of three objects. Why do we insist on objects
storing themselves then? It still may be perfectly valid pattern -- as I said real life is not
always a good answer to our modelling needs -- but it is often overused. While in real-life human
does it, we often have various *helper* objects for behavior, often ending with *-er*. While I see
why this is considered anti-pattern, I personally hate rules like *objects (classes) ending with
-er are evil* -- even more than ORM. (They let me think, though.)

In any case I agree with the point that we should not avoid SQL. If we use ORM we should also know
its QL (JPQL for JPA) and how it maps to SQL. We generally should not avoid of what happens down
the stack, especially when the abstraction is not perfect. ORM, no question about it, is not
a perfect abstraction.

To see a much better case against ORM let's read
[ORM is an anti-pattern](http://seldo.com/weblog/2011/08/11/orm_is_an_antipattern). Here we can
find summary of all the bad things related to ORM, there is hardly anything we can argue about
and if you read Ted Neward's post too, you can easily map the problems from one post to another.
We will go the full circle back to Martin Fowler and his
[ORM Hate](http://martinfowler.com/bliki/OrmHate.html). We simply have to accept ORM as it is
and either avoid it, or use it with its limitations, knowing that the abstraction is not perfect.
If we avoid it, we have to choose relational or object world, but we can hardly have both.

There are, of course, many other problems even if we embrace ORM as it is, after all I named many
in chapter [Questionable parts](#ch-questionable-parts).


### Is tuning-down a way out?

Using less of ORM and relying less on complex auto-magic features is a way I propose. It builds
on the premise that we should use ORM where it helps us, avoid it where it does not and know the
consequences of both styles and their interactions. It may happen that using both ways adds
complexity too, but from my experience it is not the case and not inherent problem. JPA has much
better mapping of values from DB to objects compared to the JDBC, so even if you used your entities
as dummy DTOs it is still worth it. It abstracts concrete SQL flavour away which has its benefits
-- and unless this is a real issue for more than couple of queries you can resolve the rest with
either native SQL support in the JPA, or use JDBC based solution.

Coming to relations, there may be many of them where *to-one* does not pose a problem. In that
case make your life easier and use mapping to objects. If cascade loading of *to-one* causes
problems you can try how well is `LAZY` supported by your ORM. Otherwise you have to live with it
for non-critical cases and work around it for the critical ones with queries -- we will get to this
in the chapter [Troubles with *to-one* relationships](#ch-to-one-troubles).

If your ORM allows it, you may go even lower on the abstraction scale and map raw foreign key
values instead of related objects. While the mapping part is possible with the JPA standard, it is
not enough as JPA does not allow you to join on such relationships. EclipseLink offers the last
missing ingredient and this solution is described in the chapter
[Removing *to-one* altogether](#ch-without-to-one).

This all renders ORM as a bit lower-level tool than intended, but still extremely useful. It still
allows you to generate schema from objects if you have control over your RDBMS (sometimes you
don't) or even just document your database with class diagram of entities[^cdd]. We still have to
keep unit-of-work and caching in check, but both are very useful if used well. I definitely don't
avoid using `EntityManager`, how could I? Caching is a different beast though, and deserves its
own section.

[^cdd]: While class diagram does not say exactly the same like E-R diagram I used it successfully
    to communicate design of tables to my DBA who had absolutely no problem to understand it, even
    for cases where many-to-many associative table was implied by a named line. E-R diagram was
    much easier to generate ex-post for documentation purposes.


## And there is more

This list of potential surprises is far from complete, but for the rest of the book we will narrow
our focus to relationships. We will review their mapping and how it affects querying.

I'd like to mention one more problem, kind of natural outcome of real-life software development.
JPA is implemented by couple of projects and these projects have bugs. If the bug is generally
critical, they fix it quite soon. If it's critical only for your project, they may not. Most
ORM projects are now open-sourced and you may try to fix it yourselves, although managing patches
for updated OSS project is rather painful.

It's all about how serious your trouble is -- if you can find work-around, do it. When EclipseLink
returned [empty `stream()`](https://bugs.eclipse.org/bugs/show_bug.cgi?id=433075) for lazy lists
(because officially they didn't support Java 8, but unofficially they had broken `Vector` subclass)
we simply copied the list and called `stream()` on the copy and we had utility for it. Sure it
wasn't nice, but it worked and it was very easy to remove later. When they fixed the issue we
simplified the code in the utility method and then inlined all the occurrences and it looked like
it never happened.

You may thing about switching your provider -- and JPA 2.1 brought a lot of goodies to make it
even easier as many properties in `persistence.xml` are now non-proprietary. But you still have
to go through the configuration (focus on caching especially) to make it work "as before" and then
you may get caught into *bug cross-fire*, as I call it. I used Hibernate for many years when I
joined another project using EclipseLink. After a few months we wanted to switch to Hibernate as
we discovered that most of the team was more familiar with it. But some of our JPA queries didn't
work on Hibernate because of a [bug](https://hibernate.atlassian.net/browse/HHH-9711).

So even something that should work in theory may be [quite a horror](http://wp.me/pcxwh-bT) in
practice. Hope you have tests to catch any potential bug affecting your project.