# Caching considerations {#ch-caching}

I didn't want to discuss caching too much, but later I did spread couple of sections about it
throughout the text. Later I decided to concentrate them here and be (mostly) done with it for
the sake of this book.


## Caching considerations {#caching-considerations}

[[JPspec](#bib-jpspec)] doesn't say much about caching. It says how to configure it -- starting
with `shared-cache-mode` in your `persistence.xml`. But I'd probably study caching documentation
of a particular provider, because if you don't care at all, you don't even know whether and how
you use the cache.

Without choosing `shared-cache-mode` it is up to the JPA provider and its defaults. This may render
any use of `@Cacheable` annotations useless. Currently, Hibernate typically doesn't cache by
default, while EclipseLink caches everything by default. Being oblivious to the cache (not related
to *cache-oblivious algorithms* at all) is rather dangerous, especially if your application is
not the only one running against the same database. In that case setting `shared-cache-mode`
explicitly to `NONE` is by far the best start. You may revisit your decisions later, but at least
you know what is happening.

T> ### Database caches too
T>
T> Caching happens also directly on the database level. You still incur a network round-trip but it
T> does not necessarily load data from a disk (that would be really slow) and you don't have this
T> entity-query duality -- although we don't know how complex the database cache is. In any case,
T> it is used whether we use second-level cache or not and, luckily, it is mostly totally
T> transparent for a programmer or user.
T>
T> If you don't want to dig deep you may disable ORM caches knowing that something still probably
T> caches for you. Often you can see that the first request for a page of something is slow
T> but the second requests is much faster -- even if both go to the database. Sure you don't go
T> as fast as you can, but you avoided a lot of problems, like stale data on the screen when you
T> change them in the database directly or distributed caches when you scale your application
T> while still using a single database server (until sufficient).

Probably the most important question to answer is: Is your application the sole user of
a particular database? If yes, you may safely use the cache without much thinking (or defaults,
which may be no cache as well). This does not mean that is a good idea. If you start tuning it
be prepared for a walk that may be not that easy.

Still, entity cache returning you entities by ID really quickly is a good idea because
it makes the problem of eager-loads of *to-one* relationships less serious. It doesn't fix
it though as all those entities are part of your current persistence context whether you want them
or not. We already mentioned that units of work bigger than necessary are not for free.

T> ### When JPA is not the only one caching
T>
T> Imagine you're caching returned JPA entities with JSR-107, e.g. using Spring implementation of
T> the JSR. It can be some data access method that returns single entity for a filter -- filter
T> is the key to the cache. There is one serious problem with this approach. Let's ignore that
T> we may have many keys that return the same entity, but in the cache these may be stored as
T> many instances (I'm not sure, but I doubt that identity is considered to de-duplicate values).
T> The real problem is that when the cache misses it reaches for JPA to get the entity, stores
T> it and gives it to you -- attached. But when the cache hits it returns detached entity. This
T> is highly non-deterministic situation that may lead to exceptions in better case or to
T> unexpected changes of entities in worse one, depending on the code that uses such a method.
T>
T> All in all -- don't cache managed stuff with technology that doesn't understand it.


## Second-level cache vs queries {#cache-vs-queries}

While persistence context (`EntityManager` or session) is sometimes considered a cache too, it
is merely a part of the unit-of-work pattern. The real cache sits underneath and is shared on the
level of the `EntityManagerFactory` -- or even between more of them across various JVMs in case
of distributed caches. This is called the *second-level cache*.[^slc] It is used to enhance
performance, typically by avoiding round-trips to the database. But caching has consequences.

[^slc]: Second-level cache is most popular term, used also in [[JPspec](#bib-jpspec)]. It appears
    in [[PJPA2](#bib-projpa2)] too, but *in-memory cache* is used more often there.

Caching should be transparent, but just turning it on is a kind of *premature optimization* which
-- in virtually all cases -- ends up being wrong. Any auto-magic can only go so far, and any
caching leads to potential inconsistencies. I believe most JPA users don't understand how the cache
is structured (I'm talking from my own experience too, after all). This depends on a concrete ORM,
but typically there is an *entity cache* and a *query cache*.

Entity cache helps with performance of `EntityManager.find`, or generally with loading by entity's
`@Id` attribute. But this will not help you if you accidentally obfuscate what you want with a
query, that would otherwise return the same. The provider has no way to know what entity (with what
ID) will be loaded just looking at arbitrary where conditions. This is what query cache is for.
Bulk update and deletes using JPQL go around either of these caches and the safest way how to
avoid inconsistent data is to evict all entities of the modified type from the caches. This is
often performed by the ORM provider automatically (again, check documentation and settings).

If you only work with whole entities all the time *and* nothing else accesses the database you
can be pretty sure you always get the right result from the entity cache. You may wonder how this
cache behaves in concurrent environment (like any EE/Spring application inherently is). If you
imagine it as a `Map`, even with synchronized access, you may feel the horror of getting the same
entity instance (`Dog` with the same ID) for two concurrent persistence contexts (like concurrent
HTTP requests) that subsequently modify various fields on the shared instance. Luckily, ORMs
provide each thread with its own copy of the entity. Internally they typically keep entities in
the cache in some "dehydrated" form.[^ecacheorm]

[^ecacheorm]: Tested with Hibernate and EclipseLink.


## Explicit application-level cache

TODO
* explicit application caching
* entity cache memory considerations


## Conclusion

You should be very aware of your cache setup. You should be very clear how you want to do it.
In the code it may look automagical, but it must be explicit somewhere -- your strategy must be
well known to all the people who may encounter it in any way (even without knowing).

You always trade something for something -- with caching it's typically memory for speed. Memory
can slow you down too, but in general we have plenty of it nowadays. If you distribute you enter
different world altogether and I'd think twice before going there. You must feel the need for it
and you must measure what you got (because you'll definitely get complexity).

Don't mix freely caching on multiple levels. Database cache is mostly transparent to us, but
when you mix two declarative caches you often make matters worse. Especially when you cache
entities with technology that is not aware of their lifecycle within the persistence context.

Finally, depending on what your keys for caching are you may waste a lot of memory. Entity ID
(like in second level cache) is natural and good key. But if you key on many various selectors that
may return the same entities (single or even whole collections) you may store many instances for
the same entity in the cache. That wastes memory. Knowing more about the logic between the keys
and values you may get better results with your implementation of some explicit cache on
an application level. It may require more effort but the pay-off may be significant.